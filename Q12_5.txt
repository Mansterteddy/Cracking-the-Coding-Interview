If you were designing a web crawler, how would you avoid getting into infinite loops?

为什么爬虫会陷入循环呢？很简单，当我们重新去解析一个已经解析过的网页时，就会陷入无限循环。这意味着当我们会重新访问那个网页所有的链接，然后不久后又会访问到这个网页。
最简单的例子就是：网页A包含了网页B的链接，然而网页B又包含了网页A的链接，那它们之间就会形成一个闭环。

那么我们怎么防止访问已经访问过的页面呢？答案也很简单，设置一个标志即可。整个互联网就是一个图结构，我们通常使用DFS（深度优先搜索）和BFS（广度优先搜索）进行遍历。
所以，就像遍历一个简单的图一样，将访问过的结点标记一下即可。